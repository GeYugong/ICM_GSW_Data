import pandas as pd
import requests
import os
import time
import random
import urllib3

# --- é…ç½® ---
# ç›®æ ‡ï¼šæŠ“å– 2021-2025 èµ›å­£ (å¯¹åº” Spotrac year å‚æ•° 2020-2024)
SEASONS = list(range(2021, 2026)) 
TEAM_SLUG = "golden-state-warriors"
OUTPUT_FILE = "data/gsw_salaries_5years.csv"

# --- ä»£ç†è®¾ç½® (ç«¯å£ 7897) ---
PROXIES = {
    "http": "http://127.0.0.1:7897",
    "https": "http://127.0.0.1:7897",
}
# æ¸…é™¤ç¯å¢ƒå˜é‡å¹²æ‰°
os.environ.pop("http_proxy", None)
os.environ.pop("https_proxy", None)

# ç¦ç”¨ SSL è­¦å‘Š (å› ä¸ºæˆ‘ä»¬è¦ç”¨ verify=False)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- æµè§ˆå™¨ä¼ªè£…æ±  ---
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0"
]

def get_salaries_hardcore():
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    all_data = []

    print(f"ğŸ’° å¼€å§‹æŠ“å–è–ªèµ„æ•°æ® (æ­»ç£•æ¨¡å¼ï¼šä¸ä½¿ç”¨ä¿åº•ï¼Œç›´åˆ°æˆåŠŸ)...")

    for season in SEASONS:
        # Spotrac URL é€»è¾‘ï¼š
        # 2021 èµ›å­£ -> year/2020
        # 2025 èµ›å­£ -> year/2024
        year_param = season - 1
        url = f"https://www.spotrac.com/nba/{TEAM_SLUG}/cap/_/year/{year_param}"
        
        print(f"\n   ğŸ¯ ç›®æ ‡: {season} èµ›å­£ ({year_param}-{season}) -> {url}")
        
        success = False
        attempt = 0
        max_retries = 10  # æœ€å¤§å°è¯•æ¬¡æ•°ï¼Œé˜²æ­¢æ­»å¾ªç¯
        
        while not success and attempt < max_retries:
            attempt += 1
            # æ¯æ¬¡è¯·æ±‚éšæœºåˆ‡æ¢ User-Agent
            headers = {
                "User-Agent": random.choice(USER_AGENTS),
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
                "Referer": "https://www.spotrac.com/nba/cap/",
                "Connection": "keep-alive" # ä¿æŒè¿æ¥
            }

            try:
                # å…³é”®ä¿®æ”¹ï¼šverify=False å¿½ç•¥ SSL è¯ä¹¦éªŒè¯ï¼Œè§£å†³ SSLEOFError
                response = requests.get(url, headers=headers, proxies=PROXIES, timeout=20, verify=False)
                
                if response.status_code == 200:
                    dfs = pd.read_html(response.text)
                    
                    found_salary = False
                    # éå†æ‰€æœ‰è¡¨æ ¼å¯»æ‰¾è–ªèµ„æ•°æ®
                    for df in dfs:
                        # æ¸…æ´—åˆ—å
                        df.columns = [str(c).replace(' ', '') for c in df.columns] # å»é™¤åˆ—åç©ºæ ¼
                        
                        # æŸ¥æ‰¾åŒ…å« CapHit çš„åˆ—
                        hit_col = next((c for c in df.columns if 'CapHit' in c), None)
                        
                        if hit_col:
                            # æ¸…æ´—æ•°å€¼
                            if df[hit_col].dtype == object:
                                clean_series = df[hit_col].replace('[\$,]', '', regex=True)
                                clean_series = pd.to_numeric(clean_series, errors='coerce').fillna(0)
                            else:
                                clean_series = df[hit_col]
                            
                            # é€»è¾‘åˆ¤æ–­ï¼šå¦‚æœæ˜¯æœ‰æ•ˆçš„è–ªèµ„è¡¨ï¼Œæ€»å’Œåº”è¯¥å¾ˆå¤§
                            total_cap = clean_series.sum()
                            
                            # å‹‡å£«é˜Ÿè–ªèµ„é€šå¸¸ > 1äº¿ (100,000,000)
                            if total_cap > 100000000:
                                all_data.append({
                                    "Season": season,
                                    "Total_Salary_Expense": total_cap,
                                    "Source": "Spotrac_Scraped"
                                })
                                print(f"      âœ… [ç¬¬{attempt}æ¬¡] æŠ“å–æˆåŠŸ: ${total_cap:,.0f}")
                                success = True
                                break
                    
                    if not success:
                        print(f"      âš ï¸ [ç¬¬{attempt}æ¬¡] é¡µé¢ä¸‹è½½æˆåŠŸï¼Œä½†æœªè§£æåˆ°æœ‰æ•ˆæ€»è–ªèµ„ï¼Œå¯èƒ½æ˜¯è¡¨æ ¼ç»“æ„å˜äº†ã€‚")
                        # å¦‚æœé¡µé¢å¯¹äº†ä½†æ²¡æ•°ï¼Œå¯èƒ½éœ€è¦äººå·¥æ£€æŸ¥ï¼Œè¿™é‡Œæˆ‘ä»¬é€‰æ‹©é‡è¯•
                        raise ValueError("Data Validation Failed")

                else:
                    print(f"      âŒ [ç¬¬{attempt}æ¬¡] HTTP {response.status_code}")
                    if response.status_code == 404:
                         print("      âš ï¸ 404 Not Found, è¯¥å¹´ä»½é¡µé¢å¯èƒ½ä¸å­˜åœ¨ã€‚")
                         break # 404å°±ä¸é‡è¯•äº†

            except Exception as e:
                print(f"      âŒ [ç¬¬{attempt}æ¬¡] é”™è¯¯: {str(e)[:100]}...") # åªæ‰“å°å‰100ä¸ªå­—ç¬¦
            
            if not success:
                # å¤±è´¥åçš„é€€é¿ç­–ç•¥ï¼šä¼‘æ¯æ—¶é—´éšé‡è¯•æ¬¡æ•°å¢åŠ  (3s, 6s, 9s...)
                wait_time = attempt * 3 + random.uniform(1, 3)
                print(f"      â³ ä¼‘æ¯ {wait_time:.1f} ç§’åé‡è¯•...")
                time.sleep(wait_time)

        if not success:
            print(f"      ğŸ’€ {season} èµ›å­£å½»åº•å¤±è´¥ï¼Œå³ä½¿å°è¯•äº† {max_retries} æ¬¡ã€‚")
            # å¦‚æœä½ çœŸçš„æƒ³è¦â€œå®ç¼ºæ¯‹æ»¥â€ï¼Œè¿™é‡Œå°±ä¸æ·»åŠ ä»»ä½•æ•°æ®
            # å¦‚æœæƒ³è‡³å°‘å ä¸ªä½ï¼Œå¯ä»¥åœ¨è¿™é‡ŒåŠ ä¸ªç©ºè¡Œ

    # --- ä¿å­˜ ---
    if all_data:
        final_df = pd.DataFrame(all_data)
        final_df.to_csv(OUTPUT_FILE, index=False)
        print(f"\nğŸ’¾ çœŸå®è–ªèµ„æ•°æ®å·²ä¿å­˜è‡³: {OUTPUT_FILE}")
        print(final_df)
    else:
        print("\nâš ï¸ æœªè·å–åˆ°ä»»ä½•æ•°æ®ã€‚")

if __name__ == "__main__":
    get_salaries_hardcore()